"""
QuantAda Heuristic Parallel Bayesian Optimizer
-------------------------------------------------------------------
Copyright (c) 2026 Starry Intelligence Technology Limited. All rights reserved.

This module implements the Entropy-Based Computational Budgeting and
Mix-Score evaluation mechanism described in our IEEE Access research.

Author: Xingchen Lin (ceo@starryint.hk)
Grant: SIT-2026-Q1
-------------------------------------------------------------------
QuantAda å¯å‘å¼å¹¶è¡Œè´å¶æ–¯ä¼˜åŒ–å™¨
===============================

åŸºäº TPE (Tree-structured Parzen Estimator) ç®—æ³•çš„é«˜æ€§èƒ½å‚æ•°å¯»ä¼˜æ¡†æ¶ï¼Œ
ä¸“ä¸ºè§£å†³éå‡¸ã€é«˜ç»´çš„é‡‘èæ—¶é—´åºåˆ—å‚æ•°ä¼˜åŒ–é—®é¢˜è€Œè®¾è®¡ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
1. **è´å¶æ–¯å†…æ ¸**ï¼šåˆ©ç”¨ TPE ç®—æ³•å»ºæ¨¡ç›®æ ‡å‡½æ•°çš„åéªŒæ¦‚ç‡åˆ†å¸ƒï¼Œé«˜æ•ˆå®šä½é«˜æ½œå‚æ•°åŒºåŸŸã€‚
2. **å¯å‘å¼ç®—åŠ›è¯„ä¼°**ï¼šåŸºäºå‚æ•°ç©ºé—´å¤æ‚åº¦ï¼ˆç†µï¼‰ä¸ç¡¬ä»¶ç®—åŠ›ï¼ˆCPUæ ¸æ•°ï¼‰ï¼Œ
   é€šè¿‡éçº¿æ€§å…¬å¼åŠ¨æ€ä¼°ç®—æœ€ä½³å°è¯•æ¬¡æ•° ($N_{trials}$)ï¼Œæ‹’ç»ç›²ç›®ç©·ä¸¾ã€‚
3. **éšæœºå¹¶å‘æ¢ç´¢**ï¼šå¼•å…¥ `Constant-Liar` é‡‡æ ·ç­–ç•¥ä¸å“ˆå¸Œå»é‡æœºåˆ¶ï¼Œ
   è§£å†³å¤šæ ¸ç¯å¢ƒä¸‹çš„"å¹¶å‘è¸©è¸"é—®é¢˜ï¼Œæ¨¡æ‹Ÿé€€ç«ç‰¹æ€§ä»¥æœ‰æ•ˆè·³å‡ºå±€éƒ¨æœ€ä¼˜é™·é˜±ã€‚
4. **å·¥ç¨‹é²æ£’æ€§**ï¼šå†…ç½®è·¨å¹³å°æ–‡ä»¶é”ç®¡ç†ã€å¼‚å¸¸è‡ªåŠ¨é™çº§åŠå…¨è‡ªåŠ¨ç¯å¢ƒæ¸…ç†æœºåˆ¶ã€‚
5. **åŠ¨æ€æ»šåŠ¨è®­ç»ƒ**ï¼šæ”¯æŒåŸºäºæ—¶é—´å‘¨æœŸçš„è‡ªåŠ¨æ»šåŠ¨åˆ‡åˆ† (Walk-Forward)ï¼Œè‡ªåŠ¨æ¨æ–­è®­ç»ƒ/æµ‹è¯•çª—å£ã€‚
"""

import ast
import copy
import datetime
import importlib
import logging
import math
import multiprocessing as mp
import os
import re
import socket
import sys
import threading
import time
import traceback
import webbrowser
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import shared_memory

import numpy as np
import optuna
import optuna.visualization as vis
import pandas as pd
from optuna.samplers import TPESampler

import config
from backtest.backtester import Backtester
from common.formatters import format_float, format_recent_backtest_metrics
from common.loader import get_class_from_name, parse_period_string
from data_providers.manager import DataManager

try:
    from optuna.storages import JournalStorage
    try:
        # Optuna 4.0+ æ–°ç‰ˆè·¯å¾„
        from optuna.storages.journal import JournalFileBackend
        # ç»™å®ƒèµ·ä¸ªé€šç”¨çš„åˆ«å
        JournalFileBackendCls = JournalFileBackend
    except ImportError:
        # æ—§ç‰ˆè·¯å¾„ (å…¼å®¹è€ç¯å¢ƒ)
        from optuna.storages import JournalFileStorage
        JournalFileBackendCls = JournalFileStorage
    HAS_JOURNAL = True
except ImportError:
    HAS_JOURNAL = False

try:
    from optuna_dashboard import run_server
    HAS_DASHBOARD = True
except ImportError:
    HAS_DASHBOARD = False

# ä»¥ metric_arg ä¸ºé”®ç¼“å­˜å‡½æ•°æŒ‡é’ˆï¼Œé¿å…å¤šæŒ‡æ ‡ä¸²ç”¨åŒä¸€ä¸ªå‡½æ•°ã€‚
# key æ ¼å¼: "{default_pkg}:{metric_arg}"
_METRIC_FUNC_CACHE = {}
_FORK_SHARED_WORKER_PAYLOAD = None


def get_metric_function(metric_arg, default_pkg="metrics"):
    """
    è·å–æŒ‡æ ‡æ–¹æ³•è·¯ç”±ï¼šæ”¯æŒç»å¯¹è·¯å¾„åå°„ä¸ç¼ºçœé™çº§ã€‚

    æ”¯æŒæ ¼å¼ï¼š
    1. ç»å¯¹è·¯å¾„æ¨¡å¼: --metric a_share.turbo_assault
       -> åŠ è½½æ ¹ç›®å½• a_share åŒ…ä¸‹çš„ turbo_assault æ¨¡å—ä¸­çš„ turbo_assault / evaluate å‡½æ•°
    2. æ·±åº¦è·¯å¾„æ¨¡å¼: --metric my_private.scores.v1.assault
       -> åŠ è½½ my_private/scores/v1 åŒ…ä¸‹çš„ assault æ¨¡å—
    3. æç®€ç¼ºçœæ¨¡å¼: --metric turbo_assault (æ²¡æœ‰ç‚¹å·)
       -> é™çº§åŠ è½½ default_pkg (é»˜è®¤ metrics) ä¸‹çš„ turbo_assault.py
    """
    global _METRIC_FUNC_CACHE

    metric_arg = (metric_arg or "").strip()
    cache_key = f"{default_pkg}:{metric_arg}"

    if cache_key in _METRIC_FUNC_CACHE:
        return _METRIC_FUNC_CACHE[cache_key]

    try:
        # 1. è·¯å¾„è§£æè§£æ (è·¯ç”±åˆ†ç¦»)
        if '.' in metric_arg:
            # å­˜åœ¨ç‚¹å·ï¼Œè¯´æ˜ç”¨æˆ·ä¼ å…¥äº†å…·ä½“çš„åŒ…è·¯å¾„ã€‚ä»æœ€å³ä¾§åˆ‡åˆ†ä¸€æ¬¡ã€‚
            # ä¾‹å¦‚ "a_share.turbo_assault" -> module_path="a_share", func_name="turbo_assault"
            # ä¾‹å¦‚ "my.private.pkg.score_func" -> module_path="my.private.pkg", func_name="score_func"
            module_path, func_name = metric_arg.rsplit('.', 1)
        else:
            # æ²¡æœ‰ç‚¹å·ï¼Œè§¦å‘æç®€æ¨¡å¼ï¼Œå›é€€åˆ°é»˜è®¤çš„ metrics åŒ…
            module_path = f"{default_pkg}.{metric_arg}"
            func_name = metric_arg

        # 2. O(1) ç»å¯¹å¯»å€å¯¼å…¥
        module = importlib.import_module(module_path)

        # 3. æå–æ‰§è¡Œå‡½æ•° (æ”¯æŒåŒåå‡½æ•°æˆ– evaluate è¯­æ³•ç³–)
        if hasattr(module, func_name):
            metric_func = getattr(module, func_name)
        elif hasattr(module, "evaluate"):
            metric_func = getattr(module, "evaluate")
        else:
            raise AttributeError(f"æ¨¡å— '{module_path}' å·²åŠ è½½ï¼Œä½†æ‰¾ä¸åˆ°åä¸º '{func_name}' æˆ– 'evaluate' çš„æ‰“åˆ†å‡½æ•°ã€‚")

        _METRIC_FUNC_CACHE[cache_key] = metric_func
        return metric_func

    except ModuleNotFoundError as e:
        raise ValueError(f"[è‡´å‘½é”™è¯¯] æŒ‡æ ‡å¯»å€å¤±è´¥ï¼Œè¯·æ”¾å…¥metricsåŒ…ä¸­æˆ–pkg.funæ ¼å¼è°ƒç”¨ç§æœ‰æŒ‡æ ‡ã€‚ä¼ å…¥å‚æ•°: '{metric_arg}'ã€‚Pythonåº•å±‚æŠ¥é”™: {e}")

def is_port_in_use(port):
    """æ£€æŸ¥æœ¬åœ°ç«¯å£æ˜¯å¦è¢«å ç”¨"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(('127.0.0.1', port)) == 0


def run_optimizer_mode(args, fixed_params, risk_params, symbol_list):
    """
    è¿è¡Œä¼˜åŒ–æ¨¡å¼ä¸»æµç¨‹ï¼ˆä» run.py ä¸‹æ²‰çš„ç¼–æ’é€»è¾‘ï¼‰ã€‚

    Args:
        args: argparse è§£æç»“æœã€‚
        fixed_params: ç­–ç•¥å›ºå®šå‚æ•°ï¼ˆç”± --params è§£æï¼‰ã€‚
        risk_params: é£æ§å‚æ•°ï¼ˆç”± --risk_params è§£æï¼‰ã€‚
        symbol_list: CLI symbols åˆ—è¡¨ï¼ˆç”¨äºå…±äº«ä¸Šä¸‹æ–‡å…œåº•ï¼‰ã€‚

    Returns:
        int: è¿›ç¨‹é€€å‡ºç ï¼ˆ0=æˆåŠŸï¼›1=è¾“å…¥/åˆå§‹åŒ–é”™è¯¯ï¼‰ã€‚
    """
    print(f"\n>>> Mode: PARAMETER OPTIMIZATION (Target: {args.metric}) <<<")

    # 1. è§£æä¼ å…¥çš„ metric (æ”¯æŒå•ä¸ªæˆ–é€—å·åˆ†éš”çš„å¤šä¸ª)
    # è‡ªåŠ¨è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼Œé¿å…å‡ºç°å¦‚ "sharpe,,calmar," çš„è„è¾“å…¥
    metrics_list = [m.strip() for m in args.metric.split(',') if m.strip()]
    if not metrics_list:
        print("Error: --metric contains no valid metric after filtering empty entries.")
        return 1

    config.LOG = False
    logging.getLogger("optuna").setLevel(logging.INFO)

    try:
        opt_p_def = ast.literal_eval(args.opt_params)
    except Exception as e:
        print(f"Error parsing opt_params JSON: {e}")
        return 1

    final_reports = []
    total_metrics = len(metrics_list)
    is_multi_metric = total_metrics > 1
    explicit_params_passed = any(
        (arg == '--params') or arg.startswith('--params=')
        for arg in sys.argv[1:]
    )
    baseline_report = None
    baseline_elapsed_hours = None
    shared_context = None
    bootstrap_job = None
    dashboard_launcher_job = None
    shared_dashboard_log_file = None
    log_dir = None

    if is_multi_metric:
        log_dir = os.path.join(os.getcwd(), config.DATA_PATH, 'optuna')
        os.makedirs(log_dir, exist_ok=True)

    def build_shared_optuna_log_file(train_range, test_range, symbols):
        if not log_dir:
            return None

        name_tag = OptimizationJob.build_optuna_name_tag(
            metric="mix_score_origin,mix_score_defender,mix_score_sniper,mix_score_turbo",
            train_period=args.train_roll_period,
            test_period=args.test_roll_period,
            train_range=train_range,
            test_range=test_range,
            data_source=args.data_source,
            symbols=symbols,
            selection=args.selection,
            run_dt=datetime.datetime.now(),
            run_pid=os.getpid(),
        )
        return os.path.join(log_dir, f"optuna_{name_tag}.log")

    # å…ˆæ„å»ºä¸€æ¬¡å…±äº«ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿åŸºå‡†ä¸å¤šæŒ‡æ ‡è®­ç»ƒå¤„äºåŒä¸€æ•°æ®å®‡å®™ï¼ˆåŒä¸€é€‰è‚¡ä¸åŒä¸€æ•°æ®åˆ‡åˆ†ï¼‰
    try:
        bootstrap_args = copy.deepcopy(args)
        bootstrap_args.metric = metrics_list[0]
        bootstrap_args.auto_launch_dashboard = not is_multi_metric
        bootstrap_job = OptimizationJob(
            args=bootstrap_args,
            fixed_params=fixed_params,
            opt_params_def=opt_p_def,
            risk_params=risk_params
        )
        shared_context = bootstrap_job.export_shared_context()
        dashboard_launcher_job = bootstrap_job
        if is_multi_metric:
            shared_dashboard_log_file = build_shared_optuna_log_file(
                train_range=bootstrap_job.train_range,
                test_range=bootstrap_job.test_range,
                symbols=bootstrap_job.target_symbols,
            )
    except Exception as e:
        print(f"[è­¦å‘Š] å…±äº«ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥ï¼Œå°†é™çº§ä¸ºé€metricç‹¬ç«‹åˆå§‹åŒ–: {e}")
        if is_multi_metric and not shared_dashboard_log_file:
            fallback_train_range = (args.start_date, args.end_date)
            fallback_test_range = (args.end_date, args.end_date) if args.test_roll_period else (None, None)
            shared_dashboard_log_file = build_shared_optuna_log_file(
                train_range=fallback_train_range,
                test_range=fallback_test_range,
                symbols=symbol_list,
            )

    if explicit_params_passed:
        print("\n--- Running Baseline Backtest from --params (Recent 3Y) ---")
        baseline_start = time.time()
        try:
            if bootstrap_job is not None:
                baseline_report = bootstrap_job._run_recent_3y_backtest(copy.deepcopy(fixed_params))
            else:
                baseline_args = copy.deepcopy(args)
                baseline_args.metric = metrics_list[0]
                baseline_args.auto_launch_dashboard = not is_multi_metric
                if shared_dashboard_log_file:
                    baseline_args.shared_journal_log_file = shared_dashboard_log_file
                baseline_job = OptimizationJob(
                    args=baseline_args,
                    fixed_params=fixed_params,
                    opt_params_def=opt_p_def,
                    risk_params=risk_params
                )
                baseline_report = baseline_job._run_recent_3y_backtest(copy.deepcopy(fixed_params))
        except Exception as e:
            print(f"[è­¦å‘Š] å½“å‰åŸºå‡†å›æµ‹å¤±è´¥: {e}")
        finally:
            baseline_elapsed_hours = (time.time() - baseline_start) / 3600.0

    for idx, current_metric in enumerate(metrics_list, 1):
        print(f"\n\n{'=' * 65}")
        print(f"ğŸš€ [æŒ‡æ ‡ {idx}/{total_metrics} æ­£åœ¨è®­ç»ƒ]: {current_metric}")
        print(f"{'=' * 65}")

        # æ·±æ‹·è´ argsï¼Œç¡®ä¿ç‰©ç†éš”ç¦»
        current_args = copy.deepcopy(args)
        current_args.metric = current_metric
        current_args.auto_launch_dashboard = not is_multi_metric
        if shared_dashboard_log_file:
            current_args.shared_journal_log_file = shared_dashboard_log_file

        start_time = time.time()

        try:
            job_kwargs = {
                "args": current_args,
                "fixed_params": fixed_params,
                "opt_params_def": opt_p_def,
                "risk_params": risk_params,
            }
            if shared_context is not None:
                job_kwargs["shared_context"] = shared_context

            job = OptimizationJob(**job_kwargs)
            if dashboard_launcher_job is None:
                dashboard_launcher_job = job

            # æ‰§è¡Œä¼˜åŒ–å¹¶æ¥æ”¶è¿”å›çš„å­—å…¸æˆ˜æŠ¥
            result_dict = job.run()
            elapsed_hours = (time.time() - start_time) / 3600.0

            if result_dict and isinstance(result_dict, dict):
                result_dict['metric_name'] = current_args.metric
                result_dict['elapsed_hours'] = elapsed_hours
                result_dict['study_db'] = getattr(current_args, 'study_name', 'N/A')
                final_reports.append(result_dict)

        except Exception as e:
            print(f"\n[è‡´å‘½é”™è¯¯] æŒ‡æ ‡ '{current_metric}' è®­ç»ƒå´©æºƒ: {e}")
            traceback.print_exc()
            print(">>> å¼•æ“é˜²å®•æœºä¿æŠ¤è§¦å‘ï¼Œå¼ºè¡Œåˆ‡å…¥ä¸‹ä¸€ä¸ªæŒ‡æ ‡...")
            continue

    if final_reports or explicit_params_passed:
        print("=== è¯·å¿½ç•¥ä¸Šæ–‡æ—¥å¿—è¾“å‡ºï¼Œè¯·å°†ä¸‹æ–‡æä¾›ç»™AIè¾…åŠ©åˆ†æ ===")
        print(">>> å¤šè‡‚èµŒåšæœºè®­ç»ƒç»“æœæ±‡æ€»(MULTI-METRIC BANDIT SUMMARY)  <<<")

        header = (
            f"| {'æŒ‡æ ‡ (Metric)':<30} | {'å¹´åŒ–æ”¶ç›Š':<10} | {'å›æ’¤':<10} | "
            f"{'Calmar':<8} | {'Sharpe':<8} | {'äº¤æ˜“æ•°':<8} | {'èƒœç‡':<10} | {'PF':<8} | "
            f"{'è€—æ—¶(h)':<8} | {'æœ€ä¼˜å‚æ•° (Params)':<22} | {'å…³è”æ—¥å¿— (Log)'}"
        )
        table_width = len(header)
        print("-" * table_width)
        print(header)
        print("-" * table_width)

        if explicit_params_passed:
            baseline_recent = baseline_report or {}
            baseline_fmt = format_recent_backtest_metrics(baseline_recent)
            m_str = "å½“å‰åŸºå‡†"
            ret_str = baseline_fmt['annual_return']
            dd_str = baseline_fmt['max_drawdown']
            calmar_str = baseline_fmt['calmar_ratio']
            sharpe_str = baseline_fmt['sharpe_ratio']
            trades_str = baseline_fmt['total_trades']
            winrate_str = baseline_fmt['win_rate']
            pf_str = baseline_fmt['profit_factor']
            t_str = format_float(baseline_elapsed_hours, digits=1)
            b_str = str(fixed_params)
            db_str = "N/A"
            print(
                f"| {m_str:<30} | {ret_str:<10} | {dd_str:<10} | "
                f"{calmar_str:<8} | {sharpe_str:<8} | {trades_str:<8} | {winrate_str:<10} | {pf_str:<8} | "
                f"{t_str:<8} | {b_str:<22} | {db_str}"
            )
            if final_reports:
                print("-" * table_width)

        for r in final_reports:
            recent = r.get('recent_backtest') or {}
            recent_fmt = format_recent_backtest_metrics(recent)
            metric_name = str(r.get('metric_name', 'Unknown'))
            score_str = str(r.get('best_score', 'N/A'))
            metric_with_score = f"{metric_name} ({score_str})" if score_str != "N/A" else metric_name
            m_str = metric_with_score[:30]
            ret_str = recent_fmt['annual_return']
            dd_str = recent_fmt['max_drawdown']
            calmar_str = recent_fmt['calmar_ratio']
            sharpe_str = recent_fmt['sharpe_ratio']
            trades_str = recent_fmt['total_trades']
            winrate_str = recent_fmt['win_rate']
            pf_str = recent_fmt['profit_factor']
            t_str = format_float(r.get('elapsed_hours', 0), digits=1)
            b_str = str(r.get('best_params', 'N/A'))
            db_str = str(r.get('log_file', 'N/A'))
            print(
                f"| {m_str:<30} | {ret_str:<10} | {dd_str:<10} | "
                f"{calmar_str:<8} | {sharpe_str:<8} | {trades_str:<8} | {winrate_str:<10} | {pf_str:<8} | "
                f"{t_str:<8} | {b_str:<22} | {db_str}"
            )

        print("-" * table_width + "\n")

        if final_reports:
            print("è¯·åœ¨ Dashboard ä¸­å›æ”¾å¹¶æ’æŸ¥å­¤ç‚¹: ")
            dashboard_logs = []
            for r in final_reports:
                log_file = r.get('log_file')
                if log_file and log_file not in dashboard_logs:
                    dashboard_logs.append(log_file)
            for log_file in dashboard_logs:
                print(f"optuna-dashboard {log_file}")

            # å¤š metric åœºæ™¯åªåœ¨æœ«å°¾å¼¹ä¸€æ¬¡ Dashboardï¼ˆå…±äº« Journal å¯èšåˆå…¨éƒ¨ metricï¼‰
            if is_multi_metric and dashboard_launcher_job and dashboard_logs:
                final_log = shared_dashboard_log_file or dashboard_logs[0]
                if os.path.exists(final_log):
                    base_port = getattr(config, 'OPTUNA_DASHBOARD_PORT', 8090)
                    target_port = base_port
                    for _ in range(100):
                        if not is_port_in_use(target_port):
                            break
                        target_port += 1
                    else:
                        print(f"[Warning] Could not find an available port starting from {base_port}.")
                        target_port = base_port
                    print(f"[Info] Multi-metric training completed. Launching aggregated dashboard: {final_log}")
                    print("[Info] Dashboard will run in foreground. Analyze results, then press Ctrl-C to exit.")
                    dashboard_launcher_job._launch_dashboard(final_log, port=target_port, background=False)
                else:
                    print(f"[Warning] Aggregated dashboard log file not found: {final_log}")
        else:
            print("[è­¦å‘Š] å½“å‰ä»…æœ‰åŸºå‡†å›æµ‹ç»“æœï¼Œè®­ç»ƒæŒ‡æ ‡æœªè¿”å›ç»“æœã€‚")
    else:
        print("\n[è­¦å‘Š] æ‰€æœ‰æŒ‡æ ‡å‡æœªè¿”å›ç»“æœ")

    return 0


class OptimizationJob:
    CN_EXCHANGE_PREFIXES = {"SHSE", "SZSE", "SH", "SZ"}
    HK_EXCHANGE_PREFIXES = {"SEHK", "HK"}
    US_EXCHANGE_PREFIXES = {"NASDAQ", "NYSE", "AMEX", "ARCA", "BATS", "ISLAND", "SMART", "PINK", "US"}

    def __init__(self, args, fixed_params, opt_params_def, risk_params, shared_context=None):
        self.args = args
        self.fixed_params = fixed_params
        self.opt_params_def = opt_params_def
        self.risk_params = risk_params
        self._reset_trial_dedupe_cache()

        # å…±äº«ä¸Šä¸‹æ–‡æ¨¡å¼ï¼šå¤ç”¨é€‰è‚¡ã€æ•°æ®æŠ“å–ä¸åˆ‡åˆ†ç»“æœï¼Œç¡®ä¿å¤šæŒ‡æ ‡/åŸºå‡†å¯¹æ¯”åœ¨åŒä¸€æ•°æ®å®‡å®™ä¸‹è¿›è¡Œ
        if shared_context is not None:
            self.strategy_class = shared_context["strategy_class"]
            self.risk_control_classes = shared_context["risk_control_classes"]
            self.data_manager = shared_context["data_manager"]
            self.target_symbols = shared_context["target_symbols"]
            self.raw_datas = shared_context["raw_datas"]
            self.train_datas = shared_context["train_datas"]
            self.test_datas = shared_context["test_datas"]
            self.train_range = shared_context["train_range"]
            self.test_range = shared_context["test_range"]
            self._window_data_cache = shared_context.get("window_data_cache", {})

            # åœ¨å¤ç”¨æ•°æ®ä¸Šä¸‹æ–‡çš„å‰æä¸‹ï¼Œä»…é‡å»ºæœ¬æ¬¡ metric å¯¹åº”çš„ study_name
            self._auto_refine_study_name()
            self.has_debugged_data = False
            return

        self.strategy_class = get_class_from_name(args.strategy, ['strategies'])
        self.risk_control_classes = []
        if args.risk:
            # æ”¯æŒé€—å·åˆ†éš”
            risk_names = args.risk.split(',')
            for r_name in risk_names:
                r_name = r_name.strip()
                if r_name:
                    cls = get_class_from_name(r_name, ['risk_controls', 'strategies'])
                    self.risk_control_classes.append(cls)

        self.data_manager = DataManager()

        # Selection Logic
        self.target_symbols = []
        if self.args.selection:
            print(f"\n--- Running Selection Phase: {self.args.selection} ---")
            try:
                selector_class = get_class_from_name(self.args.selection, ['stock_selectors', 'stock_selectors_custom'])
                selector_instance = selector_class(data_manager=self.data_manager)
                selection_result = selector_instance.run_selection()
                if isinstance(selection_result, list):
                    self.target_symbols = selection_result
                elif isinstance(selection_result, pd.DataFrame):
                    self.target_symbols = selection_result.index.tolist()
                print(f"  Selector returned {len(self.target_symbols)} symbols: {self.target_symbols}")
            except Exception as e:
                print(f"Error during selection execution: {e}")
                sys.exit(1)
        else:
            if self.args.symbols:
                self.target_symbols = [s.strip() for s in self.args.symbols.split(',')]

        if not self.target_symbols:
            print("\nError: No symbols found for optimization.")
            sys.exit(1)

        self.raw_datas = self._fetch_all_data()
        self.train_datas, self.test_datas, self.train_range, self.test_range = self._split_data()
        self._window_data_cache = {}

        # æ ¹æ®å®é™…æ—¥æœŸå’Œå¸‚åœºç±»å‹è‡ªåŠ¨ç²¾ç»†åŒ– study_name
        self._auto_refine_study_name()

        self.has_debugged_data = False

    def export_shared_context(self):
        """
        å¯¼å‡ºå¯å¤ç”¨çš„ä¼˜åŒ–ä¸Šä¸‹æ–‡ï¼Œä¾›å¤šæŒ‡æ ‡ä¸²è¡Œä»»åŠ¡å¤ç”¨ï¼Œé¿å…é‡å¤é€‰è‚¡/æ‹‰æ•°å¯¼è‡´ç»“æœä¸å¯æ¯”ã€‚
        """
        return {
            "strategy_class": self.strategy_class,
            "risk_control_classes": self.risk_control_classes,
            "data_manager": self.data_manager,
            "target_symbols": self.target_symbols,
            "raw_datas": self.raw_datas,
            "train_datas": self.train_datas,
            "test_datas": self.test_datas,
            "train_range": self.train_range,
            "test_range": self.test_range,
            "window_data_cache": self._window_data_cache,
        }

    def _reset_trial_dedupe_cache(self):
        """
        è¿›ç¨‹å†…ç»“æœç¼“å­˜ï¼šå‚æ•°å“ˆå¸Œ -> è¯„åˆ†ã€‚
        ä»…ç”¨äºé¿å…åŒä¸€ worker é‡å¤è¯„ä¼°ç›¸åŒå‚æ•°ã€‚
        """
        self._completed_trial_cache = {}

    @staticmethod
    def _get_total_cpu_cores():
        return max(1, os.cpu_count() or 1)

    @classmethod
    def _resolve_worker_count(cls, requested_jobs):
        """
        å°† n_jobs è§£æä¸ºå®é™… worker æ•°ã€‚
        è§„åˆ™ï¼š
        - n_jobs > 0: æŒ‡å®š worker æ•°ï¼ˆä¸Šé™ä¸ºæœºå™¨æ€»æ ¸æ•°ï¼‰
        - n_jobs = -1: è‡ªåŠ¨ä¿ç•™ç³»ç»Ÿå†—ä½™ï¼Œworkers = C - max(2, ceil(0.15 * C))
        - n_jobs < -1: joblib é£æ ¼ï¼Œworkers = C - (abs(n_jobs) - 1)
        - n_jobs = 0 æˆ–éæ³•å€¼: é™çº§ä¸º 1
        """
        total_cores = cls._get_total_cpu_cores()
        try:
            requested_jobs = int(requested_jobs)
        except (TypeError, ValueError):
            requested_jobs = 1

        if requested_jobs == -1:
            reserved_cores = max(2, math.ceil(total_cores * 0.15))
            return max(1, total_cores - reserved_cores)

        if requested_jobs < -1:
            reserved_cores = abs(requested_jobs) - 1
            return max(1, total_cores - reserved_cores)

        if requested_jobs == 0:
            return 1

        return max(1, min(total_cores, requested_jobs))

    def _build_worker_payload(self):
        """
        æ„é€ å¤šè¿›ç¨‹ worker æ‰€éœ€çš„æœ€å°ä¸Šä¸‹æ–‡ï¼Œé¿å…ä¼ è¾“ä¸å¿…è¦å¯¹è±¡ã€‚
        """
        return {
            "args": self.args,
            "fixed_params": self.fixed_params,
            "opt_params_def": self.opt_params_def,
            "risk_params": self.risk_params,
            "train_datas": self.train_datas,
            "train_range": self.train_range,
            # spawn å­è¿›ç¨‹ä¸ä¼šç»§æ‰¿ä¸»è¿›ç¨‹è¿è¡ŒæœŸæ”¹å†™çš„ configï¼Œæ˜¾å¼é€ä¼ æ—¥å¿—å¼€å…³ã€‚
            "log_enabled": bool(getattr(config, "LOG", False)),
        }

    @staticmethod
    def _create_shared_array(array_like):
        arr = np.ascontiguousarray(array_like)
        if arr.dtype == object:
            raise TypeError("object dtype is not supported in shared memory mode")

        alloc_size = max(1, int(arr.nbytes))
        shm = shared_memory.SharedMemory(create=True, size=alloc_size)
        shm_arr = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)
        if arr.size > 0:
            np.copyto(shm_arr, arr)

        meta = {
            "name": shm.name,
            "shape": arr.shape,
        }
        if arr.dtype.names:
            # structured dtype éœ€è¦ä¿ç•™å­—æ®µæè¿°ï¼Œdtype.str ä¼šä¸¢å¤± field names
            meta["dtype_descr"] = arr.dtype.descr
        else:
            meta["dtype"] = arr.dtype.str

        return meta, shm

    @staticmethod
    def _attach_shared_array(meta):
        shm = shared_memory.SharedMemory(name=meta["name"])
        if "dtype_descr" in meta:
            dtype = np.dtype(meta["dtype_descr"])
        else:
            dtype = np.dtype(meta["dtype"])
        arr = np.ndarray(tuple(meta["shape"]), dtype=dtype, buffer=shm.buf)
        arr.setflags(write=False)
        return arr, shm

    @staticmethod
    def _cleanup_shared_segments(shm_handles, unlink=False):
        for shm in shm_handles:
            try:
                shm.close()
            except Exception:
                pass
            if unlink:
                try:
                    shm.unlink()
                except Exception:
                    pass

    @staticmethod
    def _force_shutdown_process_pool(executor, futures):
        """
        åœ¨ Ctrl-C ç­‰ä¸­æ–­åœºæ™¯ä¸‹ï¼Œå°½å¿«å›æ”¶ ProcessPoolExecutor åŠå…¶å­è¿›ç¨‹ã€‚
        """
        if executor is None:
            return

        for fut in futures or []:
            try:
                fut.cancel()
            except Exception:
                pass

        try:
            executor.shutdown(wait=False, cancel_futures=True)
        except Exception:
            pass

        # è®¿é—®ç§æœ‰å­—æ®µåšå…œåº•æ¸…ç†ï¼Œé¿å… spawn å­è¿›ç¨‹åœ¨ä¸­æ–­åç»§ç»­å ç”¨ CPUã€‚
        processes = getattr(executor, "_processes", None)
        if not processes:
            return

        for proc in list(processes.values()):
            try:
                if proc.is_alive():
                    proc.terminate()
            except Exception:
                pass

        deadline = time.time() + 2.0
        for proc in list(processes.values()):
            try:
                remain = max(0.0, deadline - time.time())
                proc.join(timeout=remain)
            except Exception:
                pass

        for proc in list(processes.values()):
            try:
                if proc.is_alive() and hasattr(proc, "kill"):
                    proc.kill()
            except Exception:
                pass

    def _build_spawn_shared_payload(self, worker_payload):
        """
        spawn æ¨¡å¼ä¸‹å°† train_datas æ”¾å…¥ shared_memoryï¼Œé¿å…ä¸ºæ¯ä¸ª worker é‡å¤åºåˆ—åŒ–å¤§å¯¹è±¡ã€‚
        """
        train_datas = worker_payload.get("train_datas") or {}
        if not train_datas:
            return worker_payload, []

        shm_handles = []
        shared_meta = {"symbols": {}}

        try:
            for symbol, df in train_datas.items():
                idx_meta, idx_shm = self._create_shared_array(df.index.to_numpy(copy=False))
                shm_handles.append(idx_shm)

                # å°†æ‰€æœ‰åˆ—å‹æˆä¸€ä¸ª structured arrayï¼Œåªå ç”¨ä¸€ä¸ªå…±äº«å†…å­˜æ®µ
                columns_meta = []
                structured_fields = []
                col_arrays = []
                for i, col in enumerate(df.columns):
                    arr = np.ascontiguousarray(df[col].to_numpy(copy=False))
                    if arr.dtype == object:
                        raise TypeError("object dtype is not supported in shared memory mode")
                    field_name = f"f{i}"
                    structured_fields.append((field_name, arr.dtype))
                    col_arrays.append((field_name, arr))
                    columns_meta.append({
                        "name": col,
                        "field": field_name,
                    })

                records = np.empty(len(df), dtype=structured_fields)
                for field_name, arr in col_arrays:
                    records[field_name] = arr
                records_meta, records_shm = self._create_shared_array(records)
                shm_handles.append(records_shm)

                symbol_meta = {
                    "index": idx_meta,
                    "index_name": df.index.name,
                    "columns": columns_meta,
                    "records": records_meta,
                }

                shared_meta["symbols"][symbol] = symbol_meta

            shared_payload = dict(worker_payload)
            shared_payload["train_datas"] = None
            shared_payload["train_datas_shared"] = shared_meta
            return shared_payload, shm_handles
        except Exception:
            self._cleanup_shared_segments(shm_handles, unlink=True)
            return worker_payload, []

    @staticmethod
    def _restore_train_datas_from_shared(shared_meta):
        train_datas = {}
        shm_handles = []

        for symbol, symbol_meta in (shared_meta or {}).get("symbols", {}).items():
            idx_arr, idx_shm = OptimizationJob._attach_shared_array(symbol_meta["index"])
            shm_handles.append(idx_shm)
            index_obj = pd.Index(idx_arr, name=symbol_meta.get("index_name"))

            records_arr, records_shm = OptimizationJob._attach_shared_array(symbol_meta["records"])
            shm_handles.append(records_shm)

            data_dict = {}
            for col_spec in symbol_meta.get("columns", []):
                data_dict[col_spec["name"]] = records_arr[col_spec["field"]]

            train_datas[symbol] = pd.DataFrame(data_dict, index=index_obj, copy=False)

        return train_datas, shm_handles

    @classmethod
    def from_worker_payload(cls, payload):
        """
        åœ¨å­è¿›ç¨‹å†…æ¢å¤å¯æ‰§è¡Œ objective çš„æœ€å° Job å®ä¾‹ã€‚
        """
        obj = cls.__new__(cls)
        obj.args = payload["args"]
        obj.fixed_params = payload["fixed_params"]
        obj.opt_params_def = payload["opt_params_def"]
        obj.risk_params = payload["risk_params"]
        obj.train_datas = payload["train_datas"]
        obj.train_range = payload["train_range"]

        obj.strategy_class = get_class_from_name(obj.args.strategy, ['strategies'])
        obj.risk_control_classes = []
        if obj.args.risk:
            for r_name in obj.args.risk.split(','):
                r_name = r_name.strip()
                if r_name:
                    rc_cls = get_class_from_name(r_name, ['risk_controls', 'strategies'])
                    obj.risk_control_classes.append(rc_cls)

        obj.has_debugged_data = False
        obj._reset_trial_dedupe_cache()
        return obj

    @staticmethod
    def _normalize_param_value(value):
        if isinstance(value, float):
            # é™åˆ¶æµ®ç‚¹æŠ–åŠ¨ï¼Œä¿è¯å“ˆå¸Œç¨³å®š
            return round(value, 12)
        if isinstance(value, list):
            return tuple(OptimizationJob._normalize_param_value(v) for v in value)
        if isinstance(value, dict):
            return tuple(sorted((k, OptimizationJob._normalize_param_value(v)) for k, v in value.items()))
        return value

    def _params_to_key(self, params_dict):
        return tuple(sorted((k, self._normalize_param_value(v)) for k, v in params_dict.items()))

    def _get_cached_trial_value(self, params_key):
        return self._completed_trial_cache.get(params_key)

    def _cache_completed_trial(self, params_key, score):
        try:
            score_val = float(score)
        except Exception:
            return

        if math.isnan(score_val) or math.isinf(score_val):
            return

        self._completed_trial_cache[params_key] = score_val

    def _run_multiprocess_optimization(self, n_jobs, n_trials, log_file, prefer_fork_cow=False):
        if not log_file:
            raise RuntimeError("Multi-process mode requires a shared JournalStorage log file.")
        if int(n_trials) <= 0:
            return

        worker_count = self._resolve_worker_count(n_jobs)
        worker_count = min(worker_count, max(1, int(n_trials)))

        base = n_trials // worker_count
        rem = n_trials % worker_count
        worker_trials = [base + (1 if i < rem else 0) for i in range(worker_count)]
        worker_trials = [x for x in worker_trials if x > 0]
        if not worker_trials:
            return

        start_method = "spawn"
        if prefer_fork_cow and sys.platform.startswith("linux"):
            try:
                mp.get_context("fork")
                start_method = "fork"
            except ValueError:
                start_method = "spawn"

        print(f"[Optimizer] Multi-process mode: launching {len(worker_trials)} workers ({start_method}).")

        worker_payload = self._build_worker_payload()
        payload_arg = worker_payload
        shared_parent_handles = []
        seed_base = int(time.time() * 1_000_000) % (2 ** 31 - 1)
        futures = []
        ctx = mp.get_context(start_method)

        global _FORK_SHARED_WORKER_PAYLOAD
        if start_method == "fork":
            # fork æ¨¡å¼ä¸‹ï¼Œå­è¿›ç¨‹ä¼šç»§æ‰¿çˆ¶è¿›ç¨‹å†…å­˜é¡µï¼ˆCopy-on-Writeï¼‰ï¼Œ
            # é¿å…å°†å¤§ä½“é‡ train_datas å†åºåˆ—åŒ–ä¼ è¾“ç»™æ¯ä¸ª workerã€‚
            _FORK_SHARED_WORKER_PAYLOAD = worker_payload
            payload_arg = None
        else:
            payload_arg, shared_parent_handles = self._build_spawn_shared_payload(worker_payload)
            if shared_parent_handles:
                print("[Optimizer] Spawn mode: train_datas shared via multiprocessing.shared_memory.")
            else:
                print("[Optimizer] Spawn mode: fallback to payload copy (non-sharable dtype detected).")

        executor = None
        interrupted = False
        try:
            executor = ProcessPoolExecutor(max_workers=len(worker_trials), mp_context=ctx)
            for worker_idx, local_trials in enumerate(worker_trials, start=1):
                futures.append(
                    executor.submit(
                        _optimize_worker_entry,
                        payload_arg,
                        self.args.study_name,
                        log_file,
                        local_trials,
                        worker_idx,
                        seed_base + worker_idx,
                    )
                )

            for fut in as_completed(futures):
                fut.result()
        except KeyboardInterrupt:
            interrupted = True
            print("\n[Optimizer] Ctrl-C detected. Forcing worker shutdown...")
            self._force_shutdown_process_pool(executor, futures)
            raise
        finally:
            if executor is not None and not interrupted:
                executor.shutdown(wait=True, cancel_futures=False)
            if start_method == "fork":
                _FORK_SHARED_WORKER_PAYLOAD = None
            self._cleanup_shared_segments(shared_parent_handles, unlink=True)

    def _fetch_all_data(self):
        print("\n--- Fetching Data for Optimization ---")

        # 1. é”šç‚¹åˆå§‹åŒ– (Anchor Point: Test End)
        req_end = self.args.end_date
        if not req_end:
            req_end = pd.Timestamp.now().strftime('%Y%m%d')
            self.args.end_date = req_end  # å›å†™

        req_start = self.args.start_date

        # 2. åŠ¨æ€å‘¨æœŸè®¡ç®— (æ”¯æŒ Train Roll + Test Roll)
        if getattr(self.args, 'train_roll_period', None):

            # A. è®¡ç®—æµ‹è¯•é›†é•¿åº¦
            test_duration = pd.Timedelta(0)
            if getattr(self.args, 'test_roll_period', None):
                offset_test = parse_period_string(self.args.test_roll_period)
                if offset_test:
                    test_duration = offset_test

            # B. è®¡ç®—è®­ç»ƒé›†é•¿åº¦
            train_duration = parse_period_string(self.args.train_roll_period)

            # C. è®¡ç®—æ€»å›æº¯èµ·ç‚¹
            if train_duration:
                anchor_dt = pd.to_datetime(str(req_end))

                # ä¾æ¬¡æ‰£é™¤ï¼šæµ‹è¯•æœŸ -> è®­ç»ƒæœŸ -> 14å¤©ç¼“å†²åŒº
                fetch_start_dt = anchor_dt - test_duration - train_duration - pd.DateOffset(days=14)

                req_start = fetch_start_dt.strftime('%Y%m%d')

                # å›å†™ start_date
                self.args.start_date = req_start

                print(f"[Auto-Fetch] Dynamic Rolling Detected:")
                print(f"  Train Roll: {self.args.train_roll_period}")
                print(f"  Test Roll:  {getattr(self.args, 'test_roll_period', 'None (Refit Mode)')}")
                print(f"  => Fetching data from {req_start} to {req_end}")

        # 3. ç»Ÿä¸€æŠ“å–çª—å£ï¼šè®­ç»ƒéœ€æ±‚ vs Recent3Y éœ€æ±‚å–æ›´æ—©èµ·ç‚¹ï¼Œç¡®ä¿åç»­å¤šæŒ‡æ ‡/åŸºå‡†å®Œå…¨å¯æ¯”
        req_fetch_start = req_start
        recent_start, _ = self._infer_recent_3y_window()
        if recent_start:
            if not req_fetch_start or pd.to_datetime(recent_start) < pd.to_datetime(req_fetch_start):
                req_fetch_start = recent_start
                print(f"[Auto-Fetch] Extended fetch window for Recent3Y consistency:")
                print(f"  Recent3Y Start: {recent_start}")
                print(f"  => Fetching data from {req_fetch_start} to {req_end}")

        datas = {}
        for symbol in self.target_symbols:
            # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
            df = self.data_manager.get_data(
                symbol,
                start_date=req_fetch_start,
                end_date=req_end,
                specified_sources=self.args.data_source,
                timeframe=self.args.timeframe,
                compression=self.args.compression,
                refresh=self.args.refresh
            )
            if df is not None and not df.empty:
                datas[symbol] = df
            else:
                print(f"Warning: No data for {symbol}, skipping.")

        if not datas:
            raise ValueError("No data fetched. Check symbols, selection or date range.")
        return datas

    def _split_data(self):
        # 1. æ˜¾å¼æŒ‡å®šæ¨¡å¼ (æœ€é«˜ä¼˜å…ˆçº§)
        if self.args.train_period and self.args.test_period:
            tr_s, tr_e = self.args.train_period.split('-')
            te_s, te_e = self.args.test_period.split('-')

            print(f"Split Mode: Explicit Period")
            print(f"  Train: {tr_s} -> {tr_e}")
            print(f"  Test:  {te_s} -> {te_e}")

            train_d = self.slice_datas(tr_s, tr_e)
            test_d = self.slice_datas(te_s, te_e)
            return train_d, test_d, (tr_s, tr_e), (te_s, te_e)

        # 2. åŠ¨æ€æ»šåŠ¨è®­ç»ƒæ¨¡å¼ (Dynamic Rolling)
        elif getattr(self.args, 'train_roll_period', None):
            train_roll = self.args.train_roll_period
            test_roll = getattr(self.args, 'test_roll_period', None)

            print(f"Split Mode: Dynamic Rolling")

            # A. ç¡®å®šæ—¶é—´é”šç‚¹ (Anchor: Test End)
            # self.args.end_date å·²ç»åœ¨ _fetch_all_data ä¸­è¡¥å…¨
            anchor_dt = pd.to_datetime(str(self.args.end_date))

            # B. è®¡ç®—åˆ‡åˆ†ç‚¹
            if test_roll:
                # æœ‰æµ‹è¯•é›†ï¼šSplit Point = End - Test Roll
                test_offset = parse_period_string(test_roll)
                split_dt = anchor_dt - test_offset
                # é˜²æ­¢è®­ç»ƒé›†ä¸æµ‹è¯•é›†åœ¨ split_dt å½“æ—¥é‡å ï¼ˆslice æ˜¯é—­åŒºé—´ï¼‰
                train_end_dt = split_dt - pd.DateOffset(days=1)
            else:
                # æ— æµ‹è¯•é›† (Refitæ¨¡å¼)ï¼šSplit Point = End
                split_dt = anchor_dt
                train_end_dt = split_dt

            # Train Start = Split Point - Train Roll
            train_offset = parse_period_string(train_roll)
            train_start_dt = split_dt - train_offset

            tr_s = train_start_dt.strftime('%Y%m%d')
            tr_e = train_end_dt.strftime('%Y%m%d')
            te_s = split_dt.strftime('%Y%m%d')
            te_e = anchor_dt.strftime('%Y%m%d')

            print(f"  [Auto-Inferred] Train Set: {tr_s} -> {tr_e} ({train_roll})")

            if test_roll:
                print(f"  [Auto-Inferred] Test Set:  {te_s} -> {te_e} ({test_roll})")
                test_d = self.slice_datas(te_s, te_e)
            else:
                print(f"  [Auto-Inferred] Test Set:  (Skipped / Production Refit Mode)")
                test_d = {}  # ç©ºæµ‹è¯•é›†

            train_d = self.slice_datas(tr_s, tr_e)

            return train_d, test_d, (tr_s, tr_e), (te_s, te_e)

        # 3. æ¯”ä¾‹åˆ‡åˆ†æ¨¡å¼
        elif self.args.train_ratio is not None:
            ratio = float(self.args.train_ratio)
            if not (0 < ratio < 1):
                raise ValueError(f"train_ratio must be between 0 and 1 (exclusive), got: {ratio}")
            print(f"Split Mode: Ratio ({ratio * 100}% Train)")

            all_dates = sorted(list(set().union(*[self.prepare_data_index(df).index for df in self.raw_datas.values()])))
            if not all_dates:
                raise ValueError("Data has no valid dates.")
            if len(all_dates) < 2:
                raise ValueError("Need at least 2 timestamps to split train/test.")

            # é‡‡ç”¨åŠå¼€åŒºé—´åˆ‡åˆ†è¯­ä¹‰ï¼š[0, split_idx) ä¸ºè®­ç»ƒï¼Œ[split_idx, n) ä¸ºæµ‹è¯•
            split_idx = int(len(all_dates) * ratio)
            split_idx = min(max(split_idx, 1), len(all_dates) - 1)

            train_end_date = all_dates[split_idx - 1]
            test_start_date = all_dates[split_idx]

            start_date_str = all_dates[0].strftime('%Y%m%d')
            split_date_str = train_end_date.strftime('%Y%m%d')
            test_start_str = test_start_date.strftime('%Y%m%d')
            end_date_str = all_dates[-1].strftime('%Y%m%d')

            print(f"  Train End: {split_date_str}")
            print(f"  Test Start: {test_start_str}")

            train_d = self.slice_datas(start_date_str, split_date_str)
            test_d = self.slice_datas(test_start_str, end_date_str)
            return train_d, test_d, (start_date_str, split_date_str), (test_start_str, end_date_str)

        # 4. å…¨é‡æ¨¡å¼ (æ— æµ‹è¯•é›†)
        else:
            print("Warning: No split method defined. Running optimization on FULL dataset.")
            return self.raw_datas, {}, (self.args.start_date, self.args.end_date), (None, None)

    @staticmethod
    def _sanitize_name_token(value, default="NA", max_len=48):
        text = str(value or "").strip()
        text = text.replace(".", "_").replace("-", "_")
        text = re.sub(r"[^0-9A-Za-z_]+", "_", text)
        text = re.sub(r"_+", "_", text).strip("_")
        if not text:
            text = default
        return text[:max_len]

    @classmethod
    def _normalize_date_tag(cls, value, default="NA"):
        if value is None:
            return default
        text = str(value).strip()
        digits = re.sub(r"[^0-9]", "", text)
        if len(digits) >= 8:
            return digits[:8]
        return cls._sanitize_name_token(text, default=default, max_len=12)

    @classmethod
    def infer_market_label(cls, symbols=None, data_source=None, selection=None):
        prefixes = set()
        symbol_list = symbols or []

        for sym in symbol_list:
            raw = str(sym or "").strip().upper()
            if not raw:
                continue

            # å¸¸è§äº¤æ˜“æ‰€å‰ç¼€æ ¼å¼: SHSE.510300 / NASDAQ.AAPL / SEHK.700
            if "." in raw:
                prefix = raw.split(".", 1)[0]
                if prefix:
                    prefixes.add(prefix)
                continue

            # æ— å‰ç¼€ä»£ç äº¤ç»™ data_source åšäºŒæ¬¡æ¨æ–­
            if raw.endswith(".SS"):
                prefixes.add("SH")
            elif raw.endswith(".SZ"):
                prefixes.add("SZ")
            elif raw.endswith(".HK"):
                prefixes.add("HK")
            elif raw.endswith("USDT") or raw.endswith("USD"):
                prefixes.add("CRYPTO")
            else:
                prefixes.add("RAW")

        if prefixes:
            if prefixes.issubset(cls.CN_EXCHANGE_PREFIXES):
                return "CN"
            if prefixes.issubset(cls.HK_EXCHANGE_PREFIXES):
                return "HK"
            if "CRYPTO" in prefixes:
                return "CRYPTO"
            if prefixes.issubset(cls.US_EXCHANGE_PREFIXES):
                return "US"
            if prefixes == {"RAW"}:
                prefixes = set()
            elif len(prefixes) == 1:
                return cls._sanitize_name_token(next(iter(prefixes)), default="MKT", max_len=12)
            else:
                return f"MIX{len(prefixes)}"

        source_hint = str(data_source or "").split(",")[0].strip().lower()
        if source_hint:
            market_by_source = {
                "tushare": "CN",
                "sxsc_tushare": "CN",
                "akshare": "CN",
                "gm": "CN",
                "ibkr": "US",
                "tiingo": "US",
                "yf": "GLOBAL",
                "csv": "LOCAL",
            }
            return market_by_source.get(
                source_hint,
                cls._sanitize_name_token(source_hint.upper(), default="MKT", max_len=12),
            )

        if selection:
            return cls._sanitize_name_token(selection, default="SEL", max_len=16)

        return "MKT"

    @classmethod
    def build_optuna_name_tag(
            cls,
            metric,
            train_period,
            test_period,
            train_range,
            test_range,
            data_source=None,
            symbols=None,
            selection=None,
            run_dt=None,
            run_pid=None,
    ):
        train_period_tag = cls._sanitize_name_token(
            str(train_period or "ALL").upper(),
            default="ALL",
            max_len=16,
        )
        test_period_raw = str(test_period).upper() if test_period else "REFIT"
        test_period_tag = cls._sanitize_name_token(test_period_raw, default="REFIT", max_len=16)
        metric_tag = cls._sanitize_name_token(metric, default="metric", max_len=36)
        market_tag = cls.infer_market_label(symbols=symbols, data_source=data_source, selection=selection)

        tr_s = cls._normalize_date_tag((train_range or (None, None))[0], default="NA")
        tr_e = cls._normalize_date_tag((train_range or (None, None))[1], default="NA")

        te_s_raw = (test_range or (None, None))[0]
        te_e_raw = (test_range or (None, None))[1]
        if te_s_raw and te_e_raw:
            te_s = cls._normalize_date_tag(te_s_raw, default="NA")
            te_e = cls._normalize_date_tag(te_e_raw, default="NA")
            test_range_tag = f"TE{te_s}-{te_e}"
        else:
            test_range_tag = "TE_REFIT"

        run_dt = run_dt or datetime.datetime.now()
        run_stamp = run_dt.strftime("%Y%m%d-%H%M%S")
        pid_stamp = str(run_pid if run_pid is not None else os.getpid())

        return (
            f"{train_period_tag}_{test_period_tag}_{metric_tag}_{market_tag}_"
            f"TR{tr_s}-{tr_e}_{test_range_tag}_RUN{run_stamp}_{pid_stamp}"
        )

    def _auto_refine_study_name(self):
        """
        åŸºäºæ—¶é—´ç»´åº¦çš„è‡ªåŠ¨åŒ–å‘½åé€»è¾‘ï¼ˆå§‹ç»ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
        æ ¼å¼ï¼š[è®­ç»ƒå‘¨æœŸ]_[æµ‹è¯•å‘¨æœŸ]_[æŒ‡æ ‡]_[å¸‚åœº]_[è®­ç»ƒé›†èŒƒå›´]_[æµ‹è¯•é›†èŒƒå›´]_[è¿è¡Œæ—¶é—´]
        """
        new_name = self.build_optuna_name_tag(
            metric=self.args.metric,
            train_period=self.args.train_roll_period,
            test_period=getattr(self.args, "test_roll_period", None),
            train_range=self.train_range,
            test_range=self.test_range,
            data_source=getattr(self.args, "data_source", None),
            symbols=self.target_symbols,
            selection=getattr(self.args, "selection", None),
            run_dt=datetime.datetime.now(),
            run_pid=os.getpid(),
        )

        print(f"[Optimizer] Auto-refining study_name (Date-Based): {new_name}")
        self.args.study_name = new_name

    def _launch_dashboard(self, log_file, port=8080, background=True):
        """
        ç›´æ¥åœ¨ä»£ç ä¸­è¿è¡Œ Optuna Dashboardã€‚
        - background=True: åå°çº¿ç¨‹æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        - background=False: å‰å°é˜»å¡æ¨¡å¼ï¼ˆæŒ‰ Ctrl-C é€€å‡ºï¼‰
        """
        if not HAS_DASHBOARD:
            print("[Warning] 'optuna-dashboard' not installed. Skipping.")
            return

        import logging
        import http.server
        import wsgiref.simple_server

        # ç›´æ¥è¦†ç›–æ ‡å‡†åº“ http.server çš„æ—¥å¿—æ–¹æ³•ï¼Œå½»åº•æ¶ˆé™¤è®¿é—®æ—¥å¿—
        def silent_log_message(self, format, *args):
            return  # ä»€ä¹ˆéƒ½ä¸åšï¼Œç›´æ¥è¿”å›

        # è¦†ç›– http.server çš„æ—¥å¿—æ–¹æ³• (bottle é»˜è®¤ server åŸºäºæ­¤)
        http.server.BaseHTTPRequestHandler.log_message = silent_log_message
        # åŒæ—¶ä¹Ÿè¦†ç›– wsgiref çš„æ—¥å¿—æ–¹æ³• (åŒé‡ä¿é™©)
        wsgiref.simple_server.WSGIRequestHandler.log_message = silent_log_message

        mode_str = "Thread Mode" if background else "Foreground Mode"
        print("\n" + "=" * 60)
        print(f">>> STARTING DASHBOARD ({mode_str}) <<<")
        print("=" * 60)

        def build_storage_and_run():
            # é™é»˜æ—¥å¿—
            loggers_to_silence = [
                "optuna",
                "optuna_dashboard",
                "sqlalchemy",
                "bottle",
                "waitress",
                "werkzeug"
            ]
            for name in loggers_to_silence:
                logging.getLogger(name).setLevel(logging.ERROR)

            # 1. åœ¨çº¿ç¨‹å†…éƒ¨åˆå§‹åŒ–å­˜å‚¨å¯¹è±¡
            # è¿™æ ·å¯ä»¥ç¡®ä¿å®ƒè¯»å–çš„æ˜¯æœ€æ–°çš„æ–‡ä»¶
            storage = JournalStorage(JournalFileBackendCls(log_file))

            # 2. å¯åŠ¨æœåŠ¡ (è¿™æ˜¯ä¸€ä¸ªé˜»å¡æ“ä½œï¼Œä¼šä¸€ç›´è¿è¡Œ)
            run_server(storage, host="127.0.0.1", port=port)

        def open_browser_later(url):
            try:
                time.sleep(1.0)
                webbrowser.open(url)
            except Exception:
                pass

        dashboard_url = f"http://127.0.0.1:{port}"
        print(f"[Success] Dashboard is running at: {dashboard_url}")

        if background:
            def start_server():
                try:
                    build_storage_and_run()
                except OSError as e:
                    if "Address already in use" in str(e) or (hasattr(e, 'winerror') and e.winerror == 10048):
                        print(f"\n[Error] Port {port} was seized by another process just now! Dashboard failed.")
                    else:
                        print(f"\n[Error] Dashboard thread failed: {e}")
                except Exception as e:
                    print(f"\n[Error] Dashboard crashed: {e}")

            # 3. åˆ›å»ºå¹¶å¯åŠ¨å®ˆæŠ¤çº¿ç¨‹
            t = threading.Thread(target=start_server, daemon=True)
            t.start()

            # 4. å°è¯•æ‰“å¼€æµè§ˆå™¨
            open_browser_later(dashboard_url)

            print("[INFO] Dashboard running in background thread.")
            print("=" * 60 + "\n")
            return

        # å‰å°æ¨¡å¼ï¼šä¸»çº¿ç¨‹é˜»å¡ï¼Œå…è®¸ç”¨æˆ·äººå·¥æ’æŸ¥å Ctrl-C é€€å‡º
        print("[INFO] Dashboard running in foreground. Press Ctrl-C to stop.")
        print("=" * 60 + "\n")
        threading.Thread(target=open_browser_later, args=(dashboard_url,), daemon=True).start()
        try:
            build_storage_and_run()
        except KeyboardInterrupt:
            print("\n[INFO] Dashboard stopped by user (Ctrl-C).")
        except OSError as e:
            if "Address already in use" in str(e) or (hasattr(e, 'winerror') and e.winerror == 10048):
                print(f"\n[Error] Port {port} was seized by another process just now! Dashboard failed.")
            else:
                print(f"\n[Error] Dashboard failed: {e}")
        except Exception as e:
            print(f"\n[Error] Dashboard crashed: {e}")

    def _estimate_n_trials(self):
        """
        å¯å‘å¼ç®—æ³•ï¼šç†µæ¨¡å‹ä¿åº• + 16æ ¸å†å²å…¬å¼æ”¾å¤§æ ¡å‡†ã€‚
        æ ¸å¿ƒå…¬å¼ï¼š
            N = max(N_entropy, N_legacy16)
            N_legacy16 = (100 + S * sqrt(d_all)) * (1 + sqrt(16))
        å…¶ä¸­ï¼š
            - N_entropy: ç†µå¤æ‚åº¦ä¼°è®¡ï¼ˆä¸æœºå™¨æ ¸æ•°è§£è€¦ï¼‰
            - N_legacy16: å‚è€ƒä½ åŸå…ˆ 16 æ ¸å…¬å¼çš„ç›®æ ‡è§„æ¨¡
            - S: å†å²å¤æ‚åº¦è¯„åˆ†ï¼ˆæ²¿ç”¨æ—§ç‰ˆè¯„åˆ†å£å¾„ï¼‰
            - d_all: æ€»å‚æ•°ç»´åº¦
        """
        entropy_nats = 0.0
        effective_dims = 0
        continuous_dims = 0
        finite_space_size = 1
        is_finite_space = True

        for _, p_cfg in self.opt_params_def.items():
            cardinality, is_finite = self._estimate_param_cardinality(p_cfg)
            cardinality = max(1, int(cardinality))

            if cardinality <= 1:
                continue

            effective_dims += 1
            entropy_nats += math.log(cardinality)

            if is_finite:
                finite_space_size *= cardinality
            else:
                continuous_dims += 1
                is_finite_space = False

        if effective_dims == 0:
            return 1

        # ç†µä¸»å¯¼ + äº¤äº’æƒ©ç½š + è¿ç»­å‚æ•°æƒ©ç½šï¼ˆKISSï¼šå¸¸æ•°å†…ç½®ï¼Œä¸æš´éœ²é…ç½®ï¼‰
        entropy_term = 80.0 * entropy_nats
        interaction_term = 35.0 * effective_dims * math.log(effective_dims + 1.0)
        continuous_term = 120.0 * continuous_dims
        floor_term = 30.0 * effective_dims

        entropy_estimated = int(round(max(floor_term, entropy_term + interaction_term + continuous_term)))
        entropy_estimated = max(1, entropy_estimated)

        # 16æ ¸å†å²å…¬å¼ï¼šæ¢å¤ä½ ä¹‹å‰å¸¸ç”¨çš„è®­ç»ƒè§„æ¨¡é‡çº§ï¼ˆçº¦ 16kï¼‰
        legacy_complexity_score = 0.0
        total_dims = max(1, len(self.opt_params_def))
        for _, p_cfg in self.opt_params_def.items():
            p_type = p_cfg.get('type')
            if p_type == 'int':
                low = int(p_cfg.get('low', 0))
                high = int(p_cfg.get('high', low))
                step = int(p_cfg.get('step', 1) or 1)
                step = max(1, step)
                range_len = abs(high - low) / step
                legacy_complexity_score += math.log(max(range_len, 2.0)) * 30.0
            elif p_type == 'float':
                # ä¸æ—§å…¬å¼ä¸€è‡´ï¼šfloat ç»Ÿä¸€å›ºå®šæƒé‡
                legacy_complexity_score += 60.0
            elif p_type == 'categorical':
                legacy_complexity_score += len(p_cfg.get('choices', [])) * 15.0
            else:
                legacy_complexity_score += 10.0

        legacy_base = 100.0 + legacy_complexity_score * math.sqrt(total_dims)

        # è·å–å½“å‰æœºå™¨çš„çœŸå®æ ¸å¿ƒæ•°ï¼Œç”¨äºåŠ¨æ€ç¼©æ”¾ç®—åŠ›é¢„ç®—
        actual_cores = self._get_total_cpu_cores()
        dynamic_core_scale = 1.0 + math.sqrt(float(actual_cores))
        legacy_dynamic_estimated = int(round(max(1.0, legacy_base * dynamic_core_scale)))

        estimated = max(entropy_estimated, legacy_dynamic_estimated)

        # æœ‰é™ç©ºé—´ä¸‹ä¸è¶…è¿‡æ€»ç»„åˆæ•°
        if is_finite_space:
            estimated = min(estimated, finite_space_size)

        print(
            "[Optimizer] n_trials estimator: "
            f"entropy={entropy_nats:.2f}, dims={effective_dims}, cont_dims={continuous_dims}, "
            f"entropy_est={entropy_estimated}, legacy_{actual_cores}cores_est={legacy_dynamic_estimated}, "
            f"finite_space={'yes' if is_finite_space else 'no'} -> {estimated}"
        )
        return estimated

    @staticmethod
    def _estimate_param_cardinality(param_cfg):
        """
        è¿”å›å‚æ•°çš„æœ‰æ•ˆç¦»æ•£åŸºæ•° K ä¸æ˜¯å¦ä¸ºæœ‰é™ç¦»æ•£ç©ºé—´ã€‚
        è¿ç»­ floatï¼ˆæ—  stepï¼‰ä½¿ç”¨è™šæ‹Ÿç¦»æ•£åŸºæ•°è¿‘ä¼¼ç†µï¼Œä¸å‚ä¸æœ‰é™ç©ºé—´ä¸Šé™ã€‚
        """
        p_type = param_cfg.get('type')

        if p_type == 'int':
            low = int(param_cfg.get('low', 0))
            high = int(param_cfg.get('high', low))
            step = int(param_cfg.get('step', 1) or 1)
            step = max(1, step)
            if high < low:
                low, high = high, low
            count = ((high - low) // step) + 1
            return max(1, count), True

        if p_type == 'float':
            low = float(param_cfg.get('low', 0.0))
            high = float(param_cfg.get('high', low))
            if high < low:
                low, high = high, low
            if abs(high - low) <= 1e-12:
                return 1, True
            step = param_cfg.get('step', None)
            if step is not None:
                try:
                    step = float(step)
                except (TypeError, ValueError):
                    step = None
            if step is not None and step > 0:
                count = int(math.floor((high - low) / step + 1e-12)) + 1
                return max(1, count), True

            # è¿ç»­ç©ºé—´ï¼šç”¨åŒºé—´å®½åº¦æ˜ å°„ä¸ºæœ‰é™â€œä¿¡æ¯æ¡¶â€è¿‘ä¼¼ç†µ
            span = max(0.0, high - low)
            virtual_bins = int(math.ceil(span * 20.0)) + 1
            virtual_bins = max(32, min(128, virtual_bins))
            return virtual_bins, False

        if p_type == 'categorical':
            choices = param_cfg.get('choices', [])
            return max(1, len(choices)), True

        return 1, True

    def _evaluate_trial_params(self, current_params):
        import math
        if not self.train_datas:
            return -9999.0

        if not self.has_debugged_data:
            print(f"\n[DEBUG] Training Data Overview (Total {len(self.train_datas)} symbols):")
            for symbol, data in self.train_datas.items():
                print(f"  - {symbol}: {len(data)} rows")
            self.has_debugged_data = True

        try:
            bt_instance = Backtester(
                datas=self.train_datas,
                strategy_class=self.strategy_class,
                params=current_params,
                start_date=self.train_range[0],
                end_date=self.train_range[1],
                cash=self.args.cash,
                commission=self.args.commission,
                risk_control_classes=self.risk_control_classes,
                risk_control_params=self.risk_params,
                timeframe=self.args.timeframe,
                compression=self.args.compression,
                enable_plot=False,
                verbose=False
            )

            bt_instance.run()

            # æ£€æŸ¥å›æµ‹æ˜¯å¦æˆåŠŸç”Ÿæˆç»“æœï¼Œé˜²æ­¢çƒ‚å‚æ•°å¯¼è‡´å¼•æ“ç©ºè½¬
            if not getattr(bt_instance, 'results', None) or len(bt_instance.results) == 0:
                return -100.0

            strat = bt_instance.results[0]

            try:
                # æ”¶ç›Šç‡ (ç™¾åˆ†æ¯”)
                total_return_pct = (bt_instance.get_custom_metric('return') or 0.0) * 100.0

                # å¤æ™®æ¯”ç‡
                sharpe = float(bt_instance.get_custom_metric('sharpe') or 0.0)
                sharpe = 0.0 if (math.isinf(sharpe) or math.isnan(sharpe)) else sharpe

                # å¡ç›æ¯”ç‡
                calmar = bt_instance.get_custom_metric('calmar') or 0.0
                calmar = 0.0 if (math.isinf(calmar) or math.isnan(calmar)) else calmar

                # äº¤æ˜“ç»Ÿè®¡åˆ†æ
                ta = strat.analyzers.getbyname('tradeanalyzer').get_analysis()
                total_trades = ta.get('total', {}).get('total', 0)
                win_rate = ta.get('won', {}).get('total', 0) / max(total_trades, 1)

                # ç›ˆäºå› å­è®¡ç®—
                won_total = ta.get('won', {}).get('pnl', {}).get('total', 0)
                lost_total = abs(ta.get('lost', {}).get('pnl', {}).get('total', 0))
                profit_factor = won_total / lost_total if lost_total > 0 else won_total

                # æœ€å¤§å›æ’¤
                mdd = strat.analyzers.getbyname('drawdown').get_analysis().get('max', {}).get('drawdown', 100.0)
                safe_mdd = max(mdd, 1.0)  # é˜²é™¤é›¶æº¢å‡º

                # è¿è¡Œæ—¶é—´æŠ˜ç®— (ç”¨äºè®¡ç®—å¹´åŒ–è¦æ±‚)
                if len(strat.data) > 0:
                    days = (strat.data.datetime.datetime(0) - strat.data.datetime.datetime(-len(strat.data) + 1)).days
                    years = max(days / 365.25, 0.1)
                else:
                    years = 1.0

            except Exception as e:
                # Analyzer è§£æå¤±è´¥ï¼Œé€šå¸¸æ„å‘³ç€å‚æ•°å¯¼è‡´äº†æ— æ³•äº¤æ˜“ï¼Œç›´æ¥åˆ¤æ­»åˆ‘
                return -100.0

            # å°è£…æ ‡å‡†åŒ–æŒ‡æ ‡å­—å…¸ï¼Œç©ºæŠ•ç»™ç§æœ‰æ‰“åˆ†æ’ä»¶
            stats = {
                'total_return_pct': total_return_pct,
                'sharpe': sharpe,
                'calmar': calmar,
                'total_trades': total_trades,
                'win_rate': win_rate,
                'profit_factor': profit_factor,
                'mdd': mdd,
                'safe_mdd': safe_mdd,
                'years': years
            }

            if self.args.metric in ['sharpe', 'calmar', 'return']:
                metric_val = bt_instance.get_custom_metric(self.args.metric)
                if metric_val == -999.0 and self.args.metric == 'sharpe':
                    ret = bt_instance.get_custom_metric('return')
                    metric_val = ret * 0.1 if ret > 0 else ret
                return metric_val

            # è§¦å‘æ’ä»¶åŒ–çš„å¤åˆæ‰“åˆ† (å…¨åŸŸåŠ¨æ€è·¯ç”±)
            else:
                try:
                    import math  # ç¡®ä¿å†…éƒ¨å¯ä»¥ä½¿ç”¨ math
                    # è·å–ç¼“å­˜çš„å†…å­˜å‡½æ•°æŒ‡é’ˆ (è°ƒç”¨æ–‡ä»¶é¡¶éƒ¨çš„è·¯ç”±é›·è¾¾)
                    metric_func = get_metric_function(self.args.metric)

                    # æ‰§è¡Œå¤–éƒ¨ç§æœ‰æ‰“åˆ†é€»è¾‘
                    final_score = metric_func(stats, strat=strat, args=self.args)

                    # å®¹é”™é™çº§ï¼šå¦‚æœç”¨æˆ·å†™çš„æ‰“åˆ†æ’ä»¶æœ‰ bug è¿”å›äº† NaN/Infï¼Œç›´æ¥ç»™æƒ©ç½šåˆ†ä¿æŠ¤å¼•æ“
                    if final_score is None or math.isnan(final_score) or math.isinf(final_score):
                        return -100.0

                    return float(final_score)

                except Exception as e:
                    # æ•è·å¤–éƒ¨æ’ä»¶æŠ›å‡ºçš„å¼‚å¸¸ï¼Œé˜²æ­¢æŸä¸€æ¬¡è¯•é”™å¯¼è‡´æ•´ä¸ª Optuna Study å´©æºƒé€€å‡º
                    return -100.0

        except Exception as e:
            import traceback
            print(f"Trial failed: {e}")
            traceback.print_exc()
            return -9999.0

    def objective(self, trial):
        current_params = copy.deepcopy(self.fixed_params)
        trial_params_dict = {}

        for param_name, config in self.opt_params_def.items():
            p_type = config.get('type')
            if p_type == 'int':
                step = config.get('step', 1)
                high = config['high']
                low = config['low']
                corrected_high = low + int((high - low) // step) * step
                val = trial.suggest_int(param_name, low, corrected_high, step=step)
            elif p_type == 'float':
                step = config.get('step', None)
                low = config['low']
                high = config['high']
                if step is not None:
                    steps = math.floor((high - low) / step + 1e-10)
                    corrected_high = low + steps * step
                    if abs(corrected_high - high) > 1e-10:
                        high = corrected_high
                val = trial.suggest_float(param_name, low, high, step=step)
            elif p_type == 'categorical':
                val = trial.suggest_categorical(param_name, config['choices'])
            else:
                val = config.get('value')

            current_params[param_name] = val
            trial_params_dict[param_name] = val

        params_key = self._params_to_key(trial_params_dict)

        cached_value = self._get_cached_trial_value(params_key)
        if cached_value is not None:
            return cached_value

        score = self._evaluate_trial_params(current_params)
        self._cache_completed_trial(params_key, score)
        return score

    def prepare_data_index(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¡®ä¿ DataFrame çš„ç´¢å¼•æ˜¯ DatetimeIndex"""
        if isinstance(df.index, pd.DatetimeIndex):
            return df

        date_cols = ['date', 'datetime', 'trade_date', 'Date', 'Datetime']
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col])
                df.set_index(col, inplace=True)
                return df

        try:
            df.index = pd.to_datetime(df.index)
            return df
        except:
            pass
        return df

    def slice_datas(self, start_date: str, end_date: str):
        """æ ¹æ®æ—¥æœŸåˆ‡åˆ†æ•°æ®å­—å…¸"""
        sliced = {}
        if not start_date and not end_date:
            return self.raw_datas

        s = pd.to_datetime(start_date) if start_date else pd.Timestamp.min
        e = pd.to_datetime(end_date) if end_date else pd.Timestamp.max

        for symbol, df in self.raw_datas.items():
            df = self.prepare_data_index(df)
            try:
                mask = (df.index >= s) & (df.index <= e)
                sub_df = df.loc[mask]
                if not sub_df.empty:
                    sliced[symbol] = sub_df
                else:
                    pass
            except Exception as e:
                print(f"Error slicing data for {symbol}: {e}")

        return sliced

    def _infer_recent_3y_window(self):
        """
        ä½¿ç”¨ä¸ CLI ç¼ºçœ start_date ä¸€è‡´çš„é€»è¾‘ï¼Œæ¨æ–­æœ€è¿‘ä¸‰å¹´åŒºé—´ã€‚
        """
        end_str = self.args.end_date or datetime.datetime.now().strftime('%Y%m%d')
        end_dt = pd.to_datetime(str(end_str))
        start_dt = end_dt - pd.DateOffset(years=3)
        return start_dt.strftime('%Y%m%d'), end_dt.strftime('%Y%m%d')

    def _fetch_datas_for_window(self, start_date: str, end_date: str):
        """
        æŒ‰æŒ‡å®šçª—å£è·å–æ•°æ®ï¼š
        1) ä¼˜å…ˆå¤ç”¨å†…å­˜ä¸­çš„ raw_datas åˆ‡ç‰‡ï¼ˆé›¶ç½‘ç»œè¯·æ±‚ï¼‰
        2) å¯¹è¦†ç›–ä¸è¶³çš„æ ‡çš„æ‰å‘ provider è¡¥æ‹‰
        3) ç»“æœåšçª—å£çº§ç¼“å­˜ï¼Œä¾›å¤šæŒ‡æ ‡/åŸºå‡†å¤ç”¨
        """
        cache_key = f"{start_date}:{end_date}"
        if cache_key in self._window_data_cache:
            print(f"[Optimizer] Reusing cached window data: {start_date} to {end_date}")
            return self._window_data_cache[cache_key]

        datas = {}
        s = pd.to_datetime(start_date) if start_date else pd.Timestamp.min
        e = pd.to_datetime(end_date) if end_date else pd.Timestamp.max

        for symbol in self.target_symbols:
            used_preloaded = False
            raw_df = self.raw_datas.get(symbol)
            if raw_df is not None and not raw_df.empty:
                prepared_df = self.prepare_data_index(raw_df)
                try:
                    has_window = (
                        len(prepared_df) > 0
                        and prepared_df.index.min() <= s
                        and prepared_df.index.max() >= e
                    )
                    if has_window:
                        mask = (prepared_df.index >= s) & (prepared_df.index <= e)
                        sliced_df = prepared_df.loc[mask]
                        if not sliced_df.empty:
                            datas[symbol] = sliced_df
                            used_preloaded = True
                except Exception:
                    pass

            if used_preloaded:
                continue

            df = self.data_manager.get_data(
                symbol,
                start_date=start_date,
                end_date=end_date,
                specified_sources=self.args.data_source,
                timeframe=self.args.timeframe,
                compression=self.args.compression,
                refresh=self.args.refresh
            )
            if df is not None and not df.empty:
                datas[symbol] = df
            else:
                print(f"[Optimizer] Warning: No recent 3Y data for {symbol}, skipping.")

        self._window_data_cache[cache_key] = datas
        return datas

    def _run_recent_3y_backtest(self, final_params):
        """
        ä¼˜åŒ–ç»“æŸåè‡ªåŠ¨æ‰§è¡Œæœ€è¿‘ä¸‰å¹´å›æµ‹ï¼Œå¹¶è¿”å›æ ¸å¿ƒæŒ‡æ ‡ç”¨äºæœ€ç»ˆæ±‡æ€»ã€‚
        """
        recent_start, recent_end = self._infer_recent_3y_window()

        print("-" * 60)
        print(f"Running Auto Backtest on Recent 3 Years: {recent_start} to {recent_end}")
        print("-" * 60)

        recent_datas = self._fetch_datas_for_window(recent_start, recent_end)
        if not recent_datas:
            print("[Optimizer] Warning: Recent 3Y backtest skipped (no valid data).")
            return None

        try:
            bt_recent = Backtester(
                datas=recent_datas,
                strategy_class=self.strategy_class,
                params=final_params,
                start_date=recent_start,
                end_date=recent_end,
                cash=self.args.cash,
                commission=self.args.commission,
                slippage=self.args.slippage,
                risk_control_classes=self.risk_control_classes,
                risk_control_params=self.risk_params,
                timeframe=self.args.timeframe,
                compression=self.args.compression,
                enable_plot=False,
                verbose=False,
            )
            bt_recent.run()
            bt_recent.display_results()

            perf = bt_recent.get_performance_metrics()
            if not perf:
                print("[Optimizer] Warning: Recent 3Y backtest finished but metrics are unavailable.")
                return None
        except Exception as e:
            print(f"[Optimizer] Warning: Recent 3Y backtest failed: {e}")
            return None

        return {
            "start_date": recent_start,
            "end_date": recent_end,
            "total_return": perf.get("total_return"),
            "annual_return": perf.get("annual_return"),
            "sharpe_ratio": perf.get("sharpe_ratio"),
            "max_drawdown": perf.get("max_drawdown"),
            "calmar_ratio": perf.get("calmar_ratio"),
            "total_trades": perf.get("total_trades"),
            "win_rate": perf.get("win_rate"),
            "profit_factor": perf.get("profit_factor"),
            "final_portfolio": perf.get("final_portfolio"),
        }

    def run(self):
        # 1. é…ç½®å­˜å‚¨ (æ”¯æŒå¤šæ ¸)
        storage = None
        n_jobs = getattr(self.args, 'n_jobs', 1)
        resolved_requested_workers = self._resolve_worker_count(n_jobs)
        auto_launch_dashboard = bool(getattr(self.args, 'auto_launch_dashboard', True))
        shared_journal_log_file = getattr(self.args, 'shared_journal_log_file', None)
        use_journal_storage = (resolved_requested_workers != 1) or bool(shared_journal_log_file)

        log_file = None

        if use_journal_storage:
            if HAS_JOURNAL:
                log_dir = os.path.join(os.getcwd(), config.DATA_PATH, 'optuna')
                os.makedirs(log_dir, exist_ok=True)

                # æ”¯æŒå¤šæŒ‡æ ‡å…±äº«åŒä¸€ä¸ª Journal æ–‡ä»¶ï¼Œä»¥ä¾¿æœ€ç»ˆåªå¼¹å‡ºä¸€ä¸ªèšåˆ Dashboard
                if shared_journal_log_file:
                    shared_dir = os.path.dirname(shared_journal_log_file)
                    if shared_dir:
                        os.makedirs(shared_dir, exist_ok=True)
                    log_file = shared_journal_log_file
                else:
                    # ä¸ºæ¯ä¸ª study åˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶ï¼Œå½»åº•æ¶ˆé™¤è·¨ä»»åŠ¡çš„é”äº‰æŠ¢
                    log_file = os.path.join(log_dir, f"optuna_{self.args.study_name}.log")

                try:
                    # å°è¯•åˆ›å»ºæ–‡ä»¶å­˜å‚¨
                    storage = JournalStorage(JournalFileBackendCls(log_file))
                    if resolved_requested_workers != 1:
                        print(
                            f"\n[Optimizer] Multi-core mode enabled "
                            f"(n_jobs={n_jobs} -> workers={resolved_requested_workers})."
                        )
                    else:
                        print(f"\n[Optimizer] JournalStorage enabled for dashboard/log persistence (n_jobs=1).")
                    print(f"[Optimizer] Using JournalStorage: {log_file}")
                except OSError as e:
                    # ä¸“é—¨æ•è· Windows æƒé™é”™è¯¯ (WinError 1314)
                    if hasattr(e, 'winerror') and e.winerror == 1314:
                        print("\n" + "!" * 60)
                        print("[ERROR] Windows Permission Error (WinError 1314)")
                        print(
                            "Multi-core optimization on Windows (using JournalStorage) requires symbolic link privileges.")
                        print("\nPLEASE TRY ONE OF THE FOLLOWING:")
                        print("  1. Run your PowerShell/Terminal as Administrator.")
                        print(
                            "  2. OR Enable 'Developer Mode' in Windows Settings (Privacy & security -> For developers).")
                        print("  3. OR Run with --n_jobs 1 to use single-core mode.")
                        print("!" * 60 + "\n")
                        sys.exit(1)
                    else:
                        raise e
            else:
                if resolved_requested_workers != 1:
                    print("\n[Warning] optuna.storages.JournalStorage not found.")
                    print("[Warning] Fallback to single-core to avoid SQLite dependency.")
                    n_jobs = 1
                    resolved_requested_workers = 1
                elif shared_journal_log_file:
                    print("\n[Warning] JournalStorage unavailable. Dashboard persistence disabled for this run.")

        # ä½¿ç”¨ TPESampler(constant_liar=True)
        # è¿™ä¼šé˜²æ­¢å¤šä¸ª Worker åŒæ—¶é‡‡æ ·åˆ°åŒä¸€ä¸ªç‚¹ï¼ˆå¹¶å‘è¸©è¸ï¼‰
        sampler = TPESampler(constant_liar=True)

        # 2. åˆ›å»º Study (åŒ…è£¹ try-except ä»¥æ•è· Windows æƒé™é”™è¯¯)
        try:
            study = optuna.create_study(
                direction='maximize',
                study_name=self.args.study_name,
                storage=storage,
                load_if_exists=True,
                sampler=sampler,
            )

            # å°†å‘½ä»¤è¡Œå‚æ•°è®°å½•åˆ° Study User Attributes
            # vars(args) å¯ä»¥å°† Namespace è½¬æ¢ä¸ºå­—å…¸ï¼Œæ–¹ä¾¿éå†
            for key, value in vars(self.args).items():
                # ä¸ºäº†é˜²æ­¢æ—¥å¿—å¹²æ‰°æˆ– token æ³„éœ²ï¼Œå¯ä»¥æ ¹æ®éœ€è¦åšç®€å•è¿‡æ»¤
                # è¿™é‡Œå°†æ‰€æœ‰å‚æ•°è½¬ä¸ºå­—ç¬¦ä¸²å­˜å‚¨ï¼Œæ–¹ä¾¿åœ¨ Dashboard å³ä¸‹è§’ç›´æ¥æŸ¥é˜…
                study.set_user_attr(key, str(value))

        except OSError as e:
            # æ•è· WinError 1314 (Symlink æƒé™ä¸è¶³)
            if hasattr(e, 'winerror') and e.winerror == 1314:
                print("\n" + "!" * 60)
                print("[WARNING] Windows Permission Error (WinError 1314).")
                print(
                    "          Multi-core optimization requires Administrator privileges to create lock files.")
                print(
                    "          è¯·ä½¿ç”¨ç®¡ç†å‘˜æƒé™è¿è¡Œç»ˆç«¯åæ‰§è¡Œï¼Œä»¥è¿›è¡Œå¤šæ ¸ä¼˜åŒ–")
                print("          >> AUTOMATICALLY FALLING BACK TO SINGLE-CORE MODE. <<")
                print("          >> è‡ªåŠ¨é™çº§ä¸ºå•æ ¸ä¼˜åŒ–æ¨¡å¼. <<")
                print("!" * 60 + "\n")

                # é™çº§ï¼šé‡ç½®ä¸ºå•æ ¸ + å†…å­˜å­˜å‚¨
                n_jobs = 1
                resolved_requested_workers = 1
                storage = None
                study = optuna.create_study(
                    direction='maximize',
                    study_name=self.args.study_name,
                    storage=None,
                    load_if_exists=True,
                    sampler=sampler,
                )
            else:
                # å…¶ä»–é”™è¯¯ç…§å¸¸æŠ›å‡º
                raise e

        # 2. ç¡®å®š n_trials
        n_trials = self.args.n_trials
        if n_trials is None:
            n_trials = self._estimate_n_trials()
            print(f"[Optimizer] Auto-inferred n_trials: {n_trials} (entropy-complexity model)")
        else:
            n_trials = int(n_trials)
            if n_trials <= 0:
                raise ValueError(f"n_trials must be a positive integer, got: {n_trials}")

        resolved_workers = self._resolve_worker_count(n_jobs)
        effective_parallel_jobs = min(resolved_workers, max(1, int(n_trials)))

        if auto_launch_dashboard and log_file and os.path.exists(log_file):
            # ç«¯å£æ£€æµ‹ä¸é€’å¢é€»è¾‘
            base_port = getattr(config, 'OPTUNA_DASHBOARD_PORT', 8090)
            target_port = base_port

            # å°è¯•å¯»æ‰¾å¯ç”¨ç«¯å£ï¼Œæœ€å¤šå°è¯• 100 æ¬¡
            for i in range(100):
                if not is_port_in_use(target_port):
                    break
                target_port += 1
            else:
                print(f"[Warning] Could not find an available port starting from {base_port}. Dashboard might fail.")

            self._launch_dashboard(log_file, port=target_port)

        print(f"\n--- Starting Optimization ({n_trials} trials, {effective_parallel_jobs} parallel jobs) ---")

        # 3. æ‰§è¡Œä¼˜åŒ–
        try:
            if effective_parallel_jobs > 1:
                self._run_multiprocess_optimization(
                    n_jobs=n_jobs,
                    n_trials=n_trials,
                    log_file=log_file,
                    prefer_fork_cow=(not auto_launch_dashboard),
                )
            else:
                # å•æ ¸/å•å¹¶è¡Œåœºæ™¯å›é€€ä¸ºå•è¿›ç¨‹çº¿ç¨‹æ¨¡å¼ï¼ˆä¸å†å²ç‰ˆæœ¬ä¸€è‡´ï¼‰
                # è¿™é‡Œä¿ç•™ Optuna çš„ n_jobs å‚æ•°å…¥å£ï¼Œé¿å…å¼ºåˆ¶å†™æ­»ä¸º 1ã€‚
                thread_jobs = max(1, min(int(n_trials), self._resolve_worker_count(n_jobs)))
                if thread_jobs != 1:
                    print(f"[Optimizer] Fallback to single-process threaded mode (n_jobs={thread_jobs}).")
                study.optimize(self.objective, n_trials=n_trials, n_jobs=thread_jobs)
        except KeyboardInterrupt:
            print("\n[Optimizer] Optimization stopped by user.")

        if len(study.trials) == 0:
            print("No trials finished.")
            return

        best_params = study.best_params
        best_value = study.best_value

        print("\n" + "=" * 60)
        print(">>> FINAL REPORT & OUT-OF-SAMPLE VALIDATION <<<")
        print("=" * 60)

        final_params = copy.deepcopy(self.fixed_params)
        final_params.update(best_params)

        print(f"Best Parameters Found (Train Set):")
        for k, v in best_params.items():
            print(f"  {k}: {v}")

        best_val_display = format_float(best_value, digits=4)
        print(f"Best Training Score ({self.args.metric}): {best_val_display}")

        if self.test_datas:
            print("-" * 60)
            print(f"Running Validation on Test Set: {self.test_range[0]} to {self.test_range[1]}")
            print("-" * 60)

            bt_test = Backtester(
                datas=self.test_datas,
                strategy_class=self.strategy_class,
                params=final_params,
                start_date=self.test_range[0],
                end_date=self.test_range[1],
                cash=self.args.cash,
                commission=self.args.commission,
                slippage=self.args.slippage,
                risk_control_classes=self.risk_control_classes,
                risk_control_params=self.risk_params,
                timeframe=self.args.timeframe,
                compression=self.args.compression,
                enable_plot=False,
                verbose=True,
            )
            bt_test.run()
        else:
            print("\n(No Test Set Configured)")

        recent_3y_metrics = self._run_recent_3y_backtest(final_params)

        print("\n" + "=" * 60)
        print(" SUMMARY OF BEST CONFIGURATION")
        print("=" * 60)
        print(f" Strategy: {self.args.strategy}")
        print(f" Params:   {final_params}")
        if recent_3y_metrics:
            recent_fmt = format_recent_backtest_metrics(recent_3y_metrics)
            print(f" Recent3Y: {recent_3y_metrics.get('start_date')} -> {recent_3y_metrics.get('end_date')}")
            print(f" Annual:   {recent_fmt['annual_return']}")
            print(f" Drawdown: {recent_fmt['max_drawdown']}")
            print(f" Calmar:   {recent_fmt['calmar_ratio']}")
            print(f" Sharpe:   {recent_fmt['sharpe_ratio']}")
            print(f" Trades:   {recent_fmt['total_trades']}")
            print(f" WinRate:  {recent_fmt['win_rate']}")
            print(f" PF:       {recent_fmt['profit_factor']}")
        print("=" * 60 + "\n")

        return {
            "best_score": best_val_display,
            "best_params": best_params,
            "trials_completed": len(study.trials),
            "log_file": log_file,
            "recent_backtest": recent_3y_metrics,
        }


def _optimize_worker_entry(worker_payload, study_name, log_file, n_trials, worker_idx, sampler_seed):
    """
    å¤šè¿›ç¨‹å­è¿›ç¨‹å…¥å£ï¼šæ¯ä¸ª worker è¿æ¥åŒä¸€ä¸ª Studyï¼Œæ‰§è¡Œå›ºå®š trial é…é¢ã€‚
    """
    if not HAS_JOURNAL:
        raise RuntimeError("JournalStorage is required for multi-process optimization.")

    if worker_payload is None:
        # fork + COW æ¨¡å¼ï¼šä»æ¨¡å—å…¨å±€ä¸­è¯»å–çˆ¶è¿›ç¨‹ç»§æ‰¿çš„ payload
        worker_payload = _FORK_SHARED_WORKER_PAYLOAD
    if worker_payload is None:
        raise RuntimeError("Worker payload is missing.")

    # åœ¨ worker å†…åŒæ­¥ä¸»è¿›ç¨‹æ—¥å¿—å¼€å…³ï¼›ç¼ºçœæŒ‰è®­ç»ƒé™éŸ³å¤„ç†ã€‚
    config.LOG = bool(worker_payload.get("log_enabled", False))

    worker_shm_handles = []
    if worker_payload.get("train_datas") is None and worker_payload.get("train_datas_shared"):
        restored_train_datas, worker_shm_handles = OptimizationJob._restore_train_datas_from_shared(
            worker_payload["train_datas_shared"]
        )
        worker_payload = dict(worker_payload)
        worker_payload["train_datas"] = restored_train_datas

    storage = JournalStorage(JournalFileBackendCls(log_file))
    sampler = TPESampler(constant_liar=True, seed=sampler_seed)

    study = optuna.create_study(
        direction='maximize',
        study_name=study_name,
        storage=storage,
        load_if_exists=True,
        sampler=sampler,
    )

    try:
        job = OptimizationJob.from_worker_payload(worker_payload)
        study.optimize(job.objective, n_trials=n_trials, n_jobs=1)
        return {"worker_idx": worker_idx, "n_trials": n_trials}
    finally:
        OptimizationJob._cleanup_shared_segments(worker_shm_handles, unlink=False)

